{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Build a Machine Learning App from Scratch\n",
    "This an *interactive notebook version* of an article originally published by my friend [Cameron Smith](https://camtsmith.com/about). The original tutorial can be found [here](https://camtsmith.com/articles/2017-10/naive-bayes-text-classification).\n",
    "\n",
    "## Introduction\n",
    "This project has three stages: \n",
    "1. The first is constructing a corpus of language data.\n",
    "2. The second is training and testing a language classifier model to predict categories. \n",
    "3. The third step is deploying the application to the web along with an API (not covered in this notebook).\n",
    "\n",
    "You can find the complete source code [here](https://github.com/camoverride/language-classifier) and moreover if you’d like a \"sneak peek at what the application looks like in the wild\", click [here](https://language-classifier-app.herokuapp.com/).\n",
    "\n",
    "## Building a Corpus\n",
    "\n",
    "In order to perform language classification, a data source is needed. A good source will have a large amount of text and accurate category labels. Wikipedia seems like a great place to start. Not only do they have a well-documented API, but it allows for language-specific querying.\n",
    "\n",
    "We'll start off by creating a list of languages to be used (prefixes were taken from [here](https://en.wikipedia.org/wiki/List_of_Wikipedias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGUAGES = ['en', 'sv', 'de', 'fr', 'nl', 'ru', 'it', 'es', 'pl', 'vi', 'pt', 'uk', 'fa', 'sco']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're using languages with large numbers of articles (but feel free to play with it). Additionally, we are selecting [Scots](https://en.wikipedia.org/wiki/Scots_language), because it’s quite similar to English and will provide an interesting challenge for our algorithm. \n",
    "\n",
    "Let's not forget to import these libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and create one object: \n",
    "**GetArticles** which will be used to grab data from Wikipedia. It also performs basic document sanitizing, such as removing punctuation, HTML tags, and citation brackets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetArticles(object):\n",
    "    \"\"\"\n",
    "    This is an object with the public method write_articles(language_id, number_of_articles,\n",
    "    db_location). This writes sanitized articles to text files in a specified location.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "\n",
    "    def _get_random_article_ids(self, language_id, number_of_articles):\n",
    "        \"\"\"\n",
    "        Makes a request for random article ids. \"rnnamespace=0\" means that only articles are chosen,\n",
    "        as opposed to user-talk pages or category pages. These ids are used by the _get_article_text\n",
    "        function to request articles.\n",
    "        \"\"\"\n",
    "        query = \\\n",
    "                        'https://' + language_id \\\n",
    "                        + '.wikipedia.org/w/api.php?format=json&action=query&list=random&rnlimit=' \\\n",
    "                        + str(number_of_articles) + '&rnnamespace=0'\n",
    "\n",
    "        # reads the response into a json object that can be iterated over\n",
    "        data = json.loads(requests.get(query).text)\n",
    "\n",
    "        # collects the ids from the json\n",
    "        ids = []\n",
    "        for article in data['query']['random']:\n",
    "            ids.append(article['id'])\n",
    "\n",
    "        return ids\n",
    "\n",
    "\n",
    "    def _get_article_text(self, language_id, article_id_list):\n",
    "        \"\"\"\n",
    "        This function takes a list of articles and yields a tuple (article_title, article_text).\n",
    "        \"\"\"\n",
    "        for idx in article_id_list:\n",
    "            idx = str(idx)\n",
    "            query = \\\n",
    "                            'https://' + language_id \\\n",
    "                            + '.wikipedia.org/w/api.php?format=json&action=query&prop=extracts&pageids=' \\\n",
    "                            + idx + '&redirects=true'\n",
    "\n",
    "            data = json.loads(requests.get(query).text)\n",
    "\n",
    "            try:\n",
    "                title = data['query']['pages'][idx]['title']\n",
    "                text_body = data['query']['pages'][idx]['extract']\n",
    "            except KeyError as error:\n",
    "                # if nothing is returned for the request, skip to the next item\n",
    "                # if it is important to download a precise number of files\n",
    "                # then this can be repeated for every error to get a new file\n",
    "                # but getting a precise number of files shouldn't matter\n",
    "                print(error)\n",
    "                continue\n",
    "\n",
    "            def clean(text):\n",
    "                \"\"\"\n",
    "                Sanitizes the document by removing HTML tags, citations, and punctuation. This\n",
    "                function can also be expanded to remove headers, footers, side-bar elements, etc.\n",
    "                \"\"\"\n",
    "                match_tag = re.compile(r'(<[^>]+>|\\[\\d+\\]|[,.\\'\\\"()])')\n",
    "                return match_tag.sub('', text)\n",
    "\n",
    "            yield title, clean(text_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it with N random articles (recommend to scrap less than 100, but feel free to break things as part of your learning). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializing object\n",
    "getdata = GetArticles()\n",
    "# getting ids of N random articles\n",
    "articles = getdata._get_random_article_ids('en', 10)\n",
    "# fetching text\n",
    "text_list = getdata._get_article_text('en', articles)\n",
    "# printing titles\n",
    "for title, text in text_list:\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "Now that we have some data to play with, it's time to choose a classification algorithm. [Naive Bayes](https://en.wikipedia.org/wiki/Naive_Bayes_classifier) seems promising. In the first half of this section, we'll explain the math behind Naive Bayes. In the second half, we'll use [Scikit](https://scikit-learn.org/stable/) to train a Multinomial Naive Bayes classifier on our data.\n",
    "\n",
    "#### Naive Bayes\n",
    "\n",
    "Naive Bayes classifiers are a family of classifiers that take inspiration from Bayes' Theorem. Most people like to memorize Bayes' theorem and go from there, but I find that it's more useful to derive Bayes' theorem instead, as it sheds some light on how the pieces fit together. This is especially important when we need to fiddle with the prior probability (which the nature of our corpus will force us to do). I'll add other languages later on, but I'll keep things simple for the purposes of this tutorial.\n",
    "\n",
    "We can think of the probability of A given B as being equivalent to the probability of the intersection of A and B divided by the probability of B, where the probability of B is further equivalent to the intersection of A and B plus the intersection of B and not A (see Cameron's post [*Math to Code*](https://camtsmith.com/articles/2017-12/math-to-code) for a different perspective):\n",
    "\n",
    "\\begin{equation*}\n",
    "P(A | B) = {\\dfrac{P(A \\cap B)}{P(B)}} = {\\dfrac{P(A \\cap B)}{P(A \\cap B) + P(B \\cap \\overline{A})}}\n",
    "\\end{equation*}\n",
    "\n",
    "The formula above is implemented with logic, but it's more useful to convert this to math so that we can play around with specific quantities. The intersection of two sets, A and B, is the same as the probability of B given A multiplied by the probability of A. The middle section below is the canonical form of Bayes' Theorem:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(A | B) = {\\dfrac{P(B|A) \\cdot P(A)}{P(B)}} = {\\dfrac{P(B|A) \\cdot P(A)}{ P(B|A) \\cdot P(A) + P(B|\\overline{A}) \\cdot P(\\overline{A})  }}\n",
    "\\end{equation*}\n",
    "\n",
    "The intuition behind Naive Bayes is quite simple. Let's say we have three documents, one is English and contains \\[\"auld\", \"man\", \"girl\"\\] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en = [\"auld\", \"man\", \"girl\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and the other two are Scots and are \\[\"the\", \"auld\"\\] and \\[\"auld\", \"auld\"\\] (not the most realistic data, but it’ll suit our purposes.) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sco = [\"the\", \"auld\"] + [\"auld\", \"auld\"]\n",
    "sco"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# P(auld)\n",
    "p_auld = (en + sco).count('auld') / len(en + sco)\n",
    "# P(Scots)\n",
    "p_scot = len(sco) / len (en + sco)\n",
    "# P(English)\n",
    "p_engl = len(en) / len (en + sco)\n",
    "# P(auld|Scots)\n",
    "p_auld_scot = p_auld * p_scot\n",
    "# P(auld|English)\n",
    "p_auld_engl = p_auld * p_engl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are judging the input sentence \\[“auld”\\] (a vector of words containing only one element), then the probability of this belonging to Scots is:\n",
    "\n",
    "\\begin{equation*}\n",
    "P(Scots|auld) = {\\dfrac{P(auld|Scots) \\cdot P(Scots)}{P(auld)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_auld_scot * p_scot / p_auld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intuitively, this result makes sense. Three out of four times, the word \"auld\" appears in Scots, and documents labeled \"Scots\" occur more frequently than documents in the \"English\" category.\n",
    "\n",
    "\\begin{equation*}\n",
    "P(English|auld) = {\\dfrac{P(auld|English) \\cdot P(English)}{P(auld)}}\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_auld_engl * p_engl / p_auld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *To be continued*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
